{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPth1LUGVerf",
        "colab_type": "text"
      },
      "source": [
        "### I used [this dataset](https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech) for sentiment analysis .\n",
        "### My task is recognizing speech with sexist and racist sentiment .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPoJie6TWa7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "86a044a5-3d7c-48a1-99b9-158b15ca1149"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import nltk \n",
        "import re\n",
        "import math\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet \n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO-2A0dcah7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKsJuRoOVkF3",
        "colab_type": "text"
      },
      "source": [
        "### I useed pre-trained [google news vectors](https://code.google.com/archive/p/word2vec/) as word2vec model ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iklmPNvpXMP2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dfe2fcac-bf46-4ecd-a7e9-12ce97b6c908"
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(open('/content/drive/My Drive/parto tech/sentiment_analysis/twitter/word2vec/word2vec.bin','rb'), binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNfTI98sXM1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/parto tech/sentiment_analysis/twitter/train.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMnKo7q3XPEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_at_and_hashtag (text) :\n",
        "  text = text.replace('@user',' ')\n",
        "  text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Single character removal\n",
        "  text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text.replace('#',' ')\n",
        "  #TODO : use hashtags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE--QBtlVrLM",
        "colab_type": "text"
      },
      "source": [
        "### At cell below I developed functions to lemmatize tokens based on their POS tag using nltk library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1WxGjY0XRDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatizing with NLTK lemmatize\n",
        "\n",
        "\n",
        "def get_wordnet_pos(pos):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = pos.upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def my_lemmatizer (text) :\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized=[]\n",
        "  poses = [get_wordnet_pos(tag[0]) for tag in pos_tag(tokens)]\n",
        "  for i in range (len(tokens)):\n",
        "    lemma = lem.lemmatize(tokens[i],poses[i])\n",
        "    lemmatized.append(lemma)\n",
        "  return ' '.join(lemmatized)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60xc_LzgXV0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['tweet'] = train['tweet'].apply(remove_at_and_hashtag)\n",
        "train['tweet'] = train['tweet'].apply(my_lemmatizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yNW7NqqV0jl",
        "colab_type": "text"
      },
      "source": [
        "### sentence2vec function creates a vector for each sentence based on the mean of it's words vector. If there was a word which it was not present in the pre-trained word2vec model, I've considered zero vector for that ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRIfXtRtXWN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence2vec(row,w2v) :\n",
        "  vector = np.zeros((1,300)) \n",
        "  words = row.split()\n",
        "  count = 0 \n",
        "  for word in words :\n",
        "    try :\n",
        "      vector += w2v[word].reshape((1,300))\n",
        "      count +=1 \n",
        "    except KeyError :\n",
        "      continue \n",
        "  if count != 0 :\n",
        "    return vector/count\n",
        "  return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF4F3cEZWOVd",
        "colab_type": "text"
      },
      "source": [
        "### At four cells below, I've defined x and y ,then splited them to testing and training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_10Xlsx2ZTR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets = train.tweet.to_list()\n",
        "embedded_tweets = [sentence2vec(tweet,model) for tweet in tweets]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLyMhxPMZqiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(train.label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXznVk64BsO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,x_test , y_train,y_test = train_test_split(embedded_tweets,y,test_size=0.3,random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlIh-moADS6o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "115db972-8ef1-4a5c-8f65-f5534dbe2778"
      },
      "source": [
        "np.shape(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22373, 1, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0p9XEZuWnha",
        "colab_type": "text"
      },
      "source": [
        "### Because I used NN method for sentiment analysis using BERT word embedding, I prefered to train another model using neural net for word2vec word embedding for better comparison.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### at cell below i've defined NN model params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02JR5Qz1aGCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CNN1_FILTERS = 16\n",
        "CNN2_FILTERS = 32\n",
        "CNN3_FILTERS = 64\n",
        "DNN_UNITS = 512\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssngGYy-XrIM",
        "colab_type": "text"
      },
      "source": [
        "### Here I designed the architecture of NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyw_y-37JBNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_model = tf.keras.Sequential([\n",
        "              layers.Conv1D(filters=CNN1_FILTERS,kernel_size=3,input_shape=(300,1),activation=\"relu\"),\n",
        "              layers.MaxPool1D(),\n",
        "              layers.Conv1D(filters=CNN2_FILTERS,kernel_size=4,activation=\"relu\"),\n",
        "              layers.MaxPool1D(),\n",
        "              layers.Conv1D(filters=CNN3_FILTERS,kernel_size=6,activation=\"relu\"),\n",
        "              layers.MaxPool1D(),\n",
        "              layers.Flatten(),\n",
        "              layers.Dense(units=DNN_UNITS, activation=tf.nn.leaky_relu),\n",
        "              layers.Dropout(DROPOUT_RATE),\n",
        "              layers.Dense(units=DNN_UNITS, activation=tf.nn.leaky_relu),\n",
        "              layers.Dropout(DROPOUT_RATE),\n",
        "              layers.Dense(1,activation=\"sigmoid\")\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8LWtOdjG47n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "8f756bfa-2941-4bd7-c9cd-6fd3d12ffb62"
      },
      "source": [
        "text_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_6 (Conv1D)            (None, 298, 16)           64        \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 149, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 146, 32)           2080      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 73, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 68, 64)            12352     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_8 (MaxPooling1 (None, 34, 64)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2176)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               1114624   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 1,392,289\n",
            "Trainable params: 1,392,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBpz4cOY4_4",
        "colab_type": "text"
      },
      "source": [
        "### Because I did not find f1-score as metrics in tensorflow.keras, I copied the code from previous versions of keras and pass function as a metric param while compiling the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjeaIx6laJPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_f1(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sioaNqxyaK24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_model.compile(loss=\"binary_crossentropy\",\n",
        "                  optimizer=\"adam\",\n",
        "                  metrics=[get_f1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQOHVGrQZdEK",
        "colab_type": "text"
      },
      "source": [
        "### reshaping data for fitting into NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tol4EdbCUst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.reshape(x_train,(22373,300,1))\n",
        "x_test = np.reshape(x_test,(9589,300,1))\n",
        "y_train = np.reshape(y_train,(22373,1))\n",
        "y_test = np.reshape(y_test,(9589,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlthoxKSCKju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1ab96b60-a161-4a8a-aa71-553c8fc3431f"
      },
      "source": [
        "print(np.shape(x_train)) \n",
        "print(np.shape(y_train))\n",
        "\n",
        "print(np.shape(x_test))\n",
        "print(np.shape(y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22373, 300, 1)\n",
            "(22373, 1)\n",
            "(9589, 300, 1)\n",
            "(9589, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pydpxFtRaNjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "9341c545-deeb-4d48-8a20-30a69c2a77e4"
      },
      "source": [
        "\n",
        "text_model.fit(\n",
        "    x_train,\n",
        "    y_train ,\n",
        "    epochs=NB_EPOCHS,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1880 - get_f1: 0.2263\n",
            "Epoch 2/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1486 - get_f1: 0.3854\n",
            "Epoch 3/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1388 - get_f1: 0.4229\n",
            "Epoch 4/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1303 - get_f1: 0.4751\n",
            "Epoch 5/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1208 - get_f1: 0.5104\n",
            "Epoch 6/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1140 - get_f1: 0.5389\n",
            "Epoch 7/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1066 - get_f1: 0.5431\n",
            "Epoch 8/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.1003 - get_f1: 0.5658\n",
            "Epoch 9/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.0920 - get_f1: 0.6111\n",
            "Epoch 10/10\n",
            "700/700 [==============================] - 5s 7ms/step - loss: 0.0839 - get_f1: 0.6201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9927ef5f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmIQCJHbcgBD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f4eb953-7520-450d-e739-545692c3a183"
      },
      "source": [
        "np.shape(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9589, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8_CtXM8e2Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score , classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CYLjCSfadlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "cc0fc2d4-36d9-4e66-e13f-f6717a1a7c04"
      },
      "source": [
        "y_pred = text_model.predict_classes(x_test)\n",
        "print(f1_score(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5296442687747035\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97      8940\n",
            "           1       0.74      0.41      0.53       649\n",
            "\n",
            "    accuracy                           0.95      9589\n",
            "   macro avg       0.85      0.70      0.75      9589\n",
            "weighted avg       0.94      0.95      0.94      9589\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7EJGPFXZuYS",
        "colab_type": "text"
      },
      "source": [
        "### As we can see from output of above cell, performance has been **improved about 4%**\n"
      ]
    }
  ]
}