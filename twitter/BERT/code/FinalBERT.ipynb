{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalBERT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aSXHxIBt-u0z",
        "RxeMehDmGuuL",
        "oyvzrkxb-5ls",
        "kfz4Oprm_Blv",
        "6OMfZSpQ_Gjh",
        "fNK8BP7J_Woe",
        "aA-bGaiDAAmd",
        "BAXU6j5lAX36",
        "TqVUrWOLAmsW",
        "LkOijvJwBXJH",
        "fKnQFMyhBg_a",
        "WzWuLlv0Blbi",
        "jlgY-xK0Bw1s",
        "cXqAnYwGB-XZ",
        "xaFyokEECVHm",
        "KcdqkVgnCk3C",
        "kSkZxdxhDUOh",
        "Qsp8AMPeDgAq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSXHxIBt-u0z",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment analysis using BERT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhkMOHu5pj2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "054445c2-62d1-40d7-d4db-7bae160c5c42"
      },
      "source": [
        "!pip install tqdm  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzC4oKScmHPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "a9dd33a4-6577-44fa-bf65-9aa90a277688"
      },
      "source": [
        "!pip install tensorflow-gpu "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.31.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu) (49.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvAWD6e3ovfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ae0e1be9-c82c-4056-ba55-6abfacd30371"
      },
      "source": [
        "!pip install --upgrade grpcio "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: grpcio in /usr/local/lib/python3.6/dist-packages (1.31.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIGqD6qppky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "9cefe656-018b-4dcf-f010-cfdafd644d35"
      },
      "source": [
        "!pip install bert-for-tf2 "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.5)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzxf5bcxp5JS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecc69f13-9d3f-4797-f8e2-f37c4e831200"
      },
      "source": [
        "!pip install sentencepiece "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxeMehDmGuuL",
        "colab_type": "text"
      },
      "source": [
        "# Essential modules to be imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uLpEeiwqHOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import math\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras \n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re \n",
        "from sklearn.metrics import classification_report\n",
        "import keras.backend as K\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyvzrkxb-5ls",
        "colab_type": "text"
      },
      "source": [
        "## BERT modules "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9D7_Kb4-36R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert \n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfz4Oprm_Blv",
        "colab_type": "text"
      },
      "source": [
        "# Read data from my drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-bVC9kUBjdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/parto tech/sentiment_analysis/twitter/train.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/parto tech/sentiment_analysis/twitter/test.csv')\n",
        "train.drop('id',axis=1,inplace=True)\n",
        "test.drop('id',axis=1,inplace=True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OMfZSpQ_Gjh",
        "colab_type": "text"
      },
      "source": [
        "# Defining preprocessing functions to remove user metions, tags, single characters and multiple spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XlC1EEMW2e9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_at_and_hashtag (tweet) :\n",
        "  tweet = tweet.replace('@user',' ')\n",
        "  return tweet.replace('#',' ')\n",
        "\n",
        "def data_processing(tweet): \n",
        "  # remove user mentions and tags\n",
        "  sentence = remove_at_and_hashtag(tweet)\n",
        "  \n",
        "  # Single character removal\n",
        "  sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "  # Removing multiple spaces\n",
        "  sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "  return sentence"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB6baFUhW3LZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['tweet'] = train['tweet'].apply(data_processing)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNK8BP7J_Woe",
        "colab_type": "text"
      },
      "source": [
        "# Plotting data to distinguish data balance.\n",
        "\n",
        "## Due to figure below this is unbalanced dataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0PPe2NAEJkX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "c5f1d919-a264-4b07-fbfb-0985d1a9bcf7"
      },
      "source": [
        "sns.countplot(train.label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2b1a054780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA-bGaiDAAmd",
        "colab_type": "text"
      },
      "source": [
        "# In the four cells below I have downloaded uncased_L-12_H-768_A-12 version of BERT word embedding, unzip and moving it to \"model\" folder ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QKfVVA3FINn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "ac26c289-9eac-4e56-df64-6d27fe1edd1f"
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-08 08:53:43--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  66.5MB/s    in 5.8s    \n",
            "\n",
            "2020-08-08 08:53:49 (66.5 MB/s) - ‘uncased_L-12_H-768_A-12.zip.1’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obC14ptSFTnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "7b8dbb26-4ad2-463f-bc4c-01172f3b8f82"
      },
      "source": [
        "!unzip uncased_L-12_H-768_A-12.zip.1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip.1\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DZTKb9vFour",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.makedirs('model',exist_ok=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtMQzs-AFy25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv uncased_L-12_H-768_A-12/ model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAXU6j5lAX36",
        "colab_type": "text"
      },
      "source": [
        "# Keep track of essential bert files in order to be used in my final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vZ7HBckGGwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_model_name = 'uncased_L-12_H-768_A-12'\n",
        "bert_ckpt_dir = os.path.join('model/',bert_model_name)\n",
        "bert_ckpt_file = os.path.join(bert_ckpt_dir,'bert_model.ckpt')\n",
        "bert_config_file = os.path.join(bert_ckpt_dir,'bert_config.json')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqVUrWOLAmsW",
        "colab_type": "text"
      },
      "source": [
        "# Split train dataset to train and test . \n",
        "- train_data is 0.8 of whole data . train_data's shape is (25569,2)\n",
        "- test_data is 0.2 of whole data . test_data's shape is (6394,2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1a2MDESSYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_len = (len(train) // 10) * 8\n",
        "train_data , test_data = train.loc[:train_len] , train.loc[train_len:]"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkOijvJwBXJH",
        "colab_type": "text"
      },
      "source": [
        "# Create a class for tokenizing and padding the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s-7wZchHH3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetSentimentAnalysisData :\n",
        "  DATA_COLUMN = 'tweet'\n",
        "  LABEL_COLUMN = 'label'\n",
        "\n",
        "\n",
        "  def __init__(self,train,test,tokenizer:FullTokenizer,max_seq_len=150) :\n",
        "    self.train = train \n",
        "    self.test = test \n",
        "    self.tokenizer = tokenizer \n",
        "    self.max_seq_len = 0 \n",
        "\n",
        "    ((self.x_train,self.y_train),(self.x_test,self.y_test)) = map(self._prepare,[train,test])\n",
        "    self.max_seq_len = min(self.max_seq_len,max_seq_len)\n",
        "    self.x_train,self.x_test = map(self._padding,[self.x_train,self.x_test ])\n",
        "\n",
        "\n",
        "  def _prepare(self,df) :\n",
        "    x,y = [],[] \n",
        "\n",
        "    for _ , row in tqdm(df.iterrows()) :\n",
        "      tweet , label = row[TweetSentimentAnalysisData.DATA_COLUMN] , row[TweetSentimentAnalysisData.LABEL_COLUMN] \n",
        "\n",
        "      tokens = self.tokenizer.tokenize(tweet)\n",
        "      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "\n",
        "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "      self.max_seq_len = max(self.max_seq_len,len(token_ids)) \n",
        "\n",
        "      x.append(token_ids) \n",
        "      y.append(label)\n",
        "\n",
        "    return np.array(x) , np.array(y)\n",
        "  \n",
        "\n",
        "  def _padding(self,ids) :\n",
        "    x = [] \n",
        "    \n",
        "    for input_ids in ids :\n",
        "      cut_point = min(len(input_ids),self.max_seq_len-2)\n",
        "      input_ids = input_ids[:cut_point]\n",
        "      input_ids = input_ids + [0]* (self.max_seq_len-len(input_ids))\n",
        "      x.append(np.array(input_ids))\n",
        "\n",
        "\n",
        "    return np.array(x)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKnQFMyhBg_a",
        "colab_type": "text"
      },
      "source": [
        "# Create a tokenizer using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX7lymOiMmmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir,'vocab.txt'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzWuLlv0Blbi",
        "colab_type": "text"
      },
      "source": [
        "# Apply TweetSentimentAnalysis class to train_data and test_data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGVQeyl4QnyC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6d4b229-a117-40ac-f50f-691b2c3fbfa3"
      },
      "source": [
        "data = TweetSentimentAnalysisData(train_data,test_data,tokenizer,max_seq_len=150)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25569it [00:13, 1958.27it/s]\n",
            "6394it [00:03, 1990.94it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlgY-xK0Bw1s",
        "colab_type": "text"
      },
      "source": [
        "# Designing and building the architecture of neural net and take a look at net's summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P84oUsPrNJeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model (max_seq_len,bert_config_file,bert_ckpt_file) :\n",
        "  with tf.io.gfile.GFile(bert_config_file,'r') as reader :\n",
        "    bert_config = StockBertConfig.from_json_string(reader.read())\n",
        "    bert_params = map_stock_config_to_params(bert_config)\n",
        "    bert_params.adapter_size = None \n",
        "    bert = BertModelLayer.from_params(bert_params,name='bert')\n",
        "\n",
        "\n",
        "  input_ids = keras.layers.Input(shape=(max_seq_len,),dtype='int32',name ='input_ids') \n",
        "  bert_output = bert(input_ids)\n",
        "  print(bert_output.shape)\n",
        "  cls_out = keras.layers.Lambda(lambda seq : seq[:,0,:])(bert_output)\n",
        "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "  logits = keras.layers.Dense(768,activation='tanh')(cls_out)\n",
        "  logits = keras.layers.Dropout(0.5)(logits)\n",
        "  logits = keras.layers.Dense(1,activation='sigmoid')(logits)\n",
        "\n",
        "\n",
        "  model = keras.Model(inputs=input_ids,outputs=logits)\n",
        "  model.build(input_shape=(None,max_seq_len))\n",
        "\n",
        "  load_stock_weights(bert,bert_ckpt_file)\n",
        "\n",
        "  return model \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhPCt3LkQ4S6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "4f977646-d7b2-42e0-a189-4b3125f608bd"
      },
      "source": [
        "model = create_model(data.max_seq_len,bert_config_file,bert_ckpt_file)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 132, 768)\n",
            "Done loading 196 BERT weights from: model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f2ab1dc5390> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-aDzd81X4hb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "9a141c72-ce2e-4f29-f739-e0164b37dae6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 132)]             0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 132, 768)          108890112 \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 109,481,473\n",
            "Trainable params: 109,481,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXqAnYwGB-XZ",
        "colab_type": "text"
      },
      "source": [
        "# I want to use f1-score as my NN metric param, so i copied this block of code from previous versions of keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wptsqQeiZrMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def get_f1(y_true, y_pred): \n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaFyokEECVHm",
        "colab_type": "text"
      },
      "source": [
        "# Compiling model with Adam optimizer . using of Adam optimizer is suggested !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0cfZUXDYbp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(1e-5),\n",
        "    loss = keras.losses.BinaryCrossentropy() ,\n",
        "    metrics = [get_f1] ,\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcdqkVgnCk3C",
        "colab_type": "text"
      },
      "source": [
        "# As we see in previous cells this dataset is unbalanced thus I will pass weight of each class to my model. Below I've calculate the weights using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BF7frLay2aH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',classes=[0,1],y=train.label)\n",
        "class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pQ6R3Kqzi1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b05d50a-6b17-4f1f-bfe3-aea8acd3ae65"
      },
      "source": [
        "class_weights"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.5377187079407806, 1: 7.128010704727921}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSkZxdxhDUOh",
        "colab_type": "text"
      },
      "source": [
        "# Fitting the model for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g22cLnQaZxC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "61db6d15-771e-455a-a0c6-3f60d358666e"
      },
      "source": [
        "model.fit(\n",
        "    x=data.x_train ,\n",
        "    y=data.y_train ,\n",
        "    validation_split=0.1 ,\n",
        "    batch_size = 16 ,\n",
        "    shuffle = True ,\n",
        "    epochs = 5 ,\n",
        "    class_weight = class_weights\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1439/1439 [==============================] - 1575s 1s/step - loss: 0.4866 - get_f1: 0.3000 - val_loss: 0.2760 - val_get_f1: 0.4043\n",
            "Epoch 2/5\n",
            "1439/1439 [==============================] - 1565s 1s/step - loss: 0.2238 - get_f1: 0.4743 - val_loss: 0.2061 - val_get_f1: 0.4559\n",
            "Epoch 3/5\n",
            "1439/1439 [==============================] - 1566s 1s/step - loss: 0.1262 - get_f1: 0.5346 - val_loss: 0.2355 - val_get_f1: 0.4485\n",
            "Epoch 4/5\n",
            "1439/1439 [==============================] - 1568s 1s/step - loss: 0.0816 - get_f1: 0.5909 - val_loss: 0.2454 - val_get_f1: 0.4536\n",
            "Epoch 5/5\n",
            "1439/1439 [==============================] - 1571s 1s/step - loss: 0.0542 - get_f1: 0.6139 - val_loss: 0.1629 - val_get_f1: 0.5093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2ab0c345c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMl-tsNTFssX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(data.x_test)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsp8AMPeDgAq",
        "colab_type": "text"
      },
      "source": [
        "# Let's take a look at classification_report of our model !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFnipG8FGe7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "943aa07f-e67d-4b6c-b6a9-e28c3d84f9de"
      },
      "source": [
        "print(classification_report(data.y_test,K.round(y_pred)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98      5950\n",
            "           1       0.64      0.82      0.72       444\n",
            "\n",
            "    accuracy                           0.96      6394\n",
            "   macro avg       0.81      0.89      0.85      6394\n",
            "weighted avg       0.96      0.96      0.96      6394\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cneyMBtVDm9F",
        "colab_type": "text"
      },
      "source": [
        "# Compared with word2vec using NN approach, f1-score of positive class (contains sexism or racism content) has been improved by 23% !!\n",
        "# F1-score of negative class is still 98%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzTrYFpdFKCk",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion \n",
        "### By using BERT word embedding accuracy has imporved so much but it takes about one hour to learn because NN has more than 109 million trainable parameters"
      ]
    }
  ]
}