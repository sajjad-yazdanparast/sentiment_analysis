{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1eBMRwsrXGuqJ7he5zxWLnAhLPXIt9PQw",
      "authorship_tag": "ABX9TyOdg4aCHC+gvfUw8/wkrj5F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajjad-yazdanparast/sentiment_analysis/blob/master/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OK8T8-eJmnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "460a20ba-f127-4c7f-b7c7-ab37b49907ba"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import nltk \n",
        "import spacy\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooP90eumBIPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def gunzip(file_path,output_path):\n",
        "#     with gzip.open(file_path,\"rb\") as f_in, open(output_path,\"wb\") as f_out:\n",
        "#         shutil.copyfileobj(f_in, f_out)\n",
        "# gunzip('/content/drive/My Drive/parto tech/ GoogleNews-vectors-negative300.bin.gz','/content/drive/My Drive/parto tech/word2vec.bin')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHNlkWcDNnE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3d36ac77-6772-4302-bc4c-9db02edae2d2"
      },
      "source": [
        "model = KeyedVectors.load_word2vec_format(open('/content/drive/My Drive/parto tech/sentiment_analysis/word2vec.bin','rb'), binary=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8JDWxsNp95k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/parto tech/sentiment_analysis/train.csv') \n",
        "test = pd.read_csv('/content/drive/My Drive/parto tech/sentiment_analysis/test.csv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukg557QUs7gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_at_and_hashtag (text) :\n",
        "  text = text.replace('@user',' ')\n",
        "  return text.replace('#',' ')\n",
        "  #TODO : use hashtags"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNOwvxPC91fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatizing with NLTK lemmatizer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet \n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "def my_lemmatizer (text) :\n",
        "  tokens = word_tokenize(text)\n",
        "  lemmatized=[]\n",
        "  poses = [get_wordnet_pos(tag[0]) for tag in pos_tag(tokens)]\n",
        "  for i in range (len(tokens)):\n",
        "    lemma = lem.lemmatize(tokens[i],poses[i])\n",
        "    lemmatized.append(lemma)\n",
        "  return ' '.join(lemmatized)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L5RPcX0MOm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def my_lemmtizer(text) :\n",
        "#   nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "#   doc = nlp(text)\n",
        "#   string = \" \".join([token.lemma_ for token in doc])\n",
        "#   return re.sub('-PRON-','they',string)\n",
        " "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkP1IlxeqMUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['tweet'] = train['tweet'].apply(remove_at_and_hashtag)\n",
        "train['tweet'] = train['tweet'].apply(my_lemmatizer)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXLUMQnHuSf2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence2vec(row,w2v) :\n",
        "  vector = np.zeros((1,300)) \n",
        "  words = row.split()\n",
        "  count = 0 \n",
        "  for word in words :\n",
        "    try :\n",
        "      vector += w2v[word].reshape((1,300))\n",
        "      count +=1 \n",
        "    except KeyError :\n",
        "      continue \n",
        "  if count != 0 :\n",
        "    return vector/count\n",
        "  return vector"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSy2Qpm2W1hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hated_sentences = train[train['label']==1]['tweet']\n",
        "positive_sentences = train[train['label']==0]['tweet']\n",
        "hated_vecs = np.concatenate([sentence2vec(z,model) for z in hated_sentences])\n",
        "positive_vecs = np.concatenate([sentence2vec(z,model) for z in positive_sentences])\n",
        "from sklearn.manifold import TSNE\n",
        "ts = TSNE (2)\n",
        "hated_vecs = ts.fit_transform(hated_vecs)\n",
        "positive_vecs = ts.fit_transform(positive_vecs)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oawhQpTQZi1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt \n",
        "# for row in positive_vecs :\n",
        "#   plt.plot(row[0],row[1],marker='o',color='g')\n",
        "# for row in hated_vecs :\n",
        "#   plt.plot(row[0],row[1],marker='o',color='r')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CryXuZojpz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "6962adde-08f0-49e4-d4f9-ba2b02f13954"
      },
      "source": [
        "# train.info()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31962 entries, 0 to 31961\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      31962 non-null  int64 \n",
            " 1   label   31962 non-null  int64 \n",
            " 2   tweet   31962 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 749.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051H_tvnfH66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.concatenate((positive_vecs,hated_vecs))\n",
        "Y = np.concatenate( (np.zeros(len(positive_vecs)) , np.ones(len(hated_vecs))))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_pXfBtj_AI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1cc0dd9-124c-435a-dac9-5f6e87892865"
      },
      "source": [
        "len(positive_vecs) / len(hated_vecs)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.256021409455842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFNtcLiPhLAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train , x_test , y_train , y_test = train_test_split(X,Y,random_state=101)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtSl8quSiB8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "14118252-ef00-445f-b46d-97c4315d0d97"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC,LinearSVC\n",
        "from sklearn.naive_bayes import *\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# model_ = LogisticRegression(random_state=101)\n",
        "model_ = SVC(gamma='auto')\n",
        "# model_ = GaussianNB()\n",
        "# model_ = XGBClassifier(n_estimators=100,)\n",
        "# model_ = RandomForestClassifier(class_weight={1:13.2560 , 0:1})\n",
        "# model_ = LinearSVC(C=1)\n",
        "model_.fit(x_train,y_train)\n",
        "y_pred = model_.predict(x_test)\n",
        "\n",
        "model_.score(x_test,y_test)\n",
        "print(classification_report(y_true=y_test,y_pred=y_pred))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      1.00      0.97      7446\n",
            "         1.0       0.90      0.13      0.23       545\n",
            "\n",
            "    accuracy                           0.94      7991\n",
            "   macro avg       0.92      0.57      0.60      7991\n",
            "weighted avg       0.94      0.94      0.92      7991\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4P8-xlGbcor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}